%% Reinforcement Learning Enhanced Genetic Algorithm for LQR Tuning
% Uses MATLAB R2024b Reinforcement Learning Toolbox with Custom Environment

%% 1. Prerequisites and Setup
clc; clear; close all;
disp('========================================================');
disp('  RL-GA Enhanced LQR Optimizer (MATLAB RL Toolbox)');
disp('========================================================');

% Load system dynamics and excitation
generate_nsae_kanai_tajimi_2023Brandao;
Dynamic_model; 

% Define problem parameters
n = size(Ms, 1);
N_STATES = 2 * n;
N_CONTROLS = size(Gamma, 2);
F_MR_MAX = 2.0e4;
F_MR_MIN = -F_MR_MAX;

input_excite = timeseries(ag, t);
ModelName = 'WOA_vs_RLGA'; %--> 

% Search bounds (log10 scale)
LB = [4, -8];
UB = [10, 2];

%% 2. CREATING CUSTOM RL ENVIRONMENT
disp('1. Creating RL Environment...');

% Ssave the environment class to a separate file
createEnvironmentClass();

% Create environment instance
env = LQROptimizationEnv(LB, UB, A, Bc, E, F_MR_MAX, input_excite, ...
    N_STATES, N_CONTROLS, ModelName);

disp('******  Environment created successfully  ******');

%% 3. CREATE DDPG AGENT
disp('2. Building DDPG Agent...');

obsInfo = getObservationInfo(env);
actInfo = getActionInfo(env);

% Critic Network
statePath = [
    featureInputLayer(2, 'Normalization', 'none', 'Name', 'state')
    fullyConnectedLayer(128, 'Name', 'fc_s1')
    reluLayer('Name', 'relu_s1')
    fullyConnectedLayer(64, 'Name', 'fc_s2')
    reluLayer('Name', 'relu_s2')];

actionPath = [
    featureInputLayer(2, 'Normalization', 'none', 'Name', 'action')
    fullyConnectedLayer(64, 'Name', 'fc_a1')];

commonPath = [
    additionLayer(2, 'Name', 'add')
    reluLayer('Name', 'relu_common')
    fullyConnectedLayer(32, 'Name', 'fc_common')
    reluLayer('Name', 'relu_fc')
    fullyConnectedLayer(1, 'Name', 'QValue')];

criticNet = layerGraph(statePath);
criticNet = addLayers(criticNet, actionPath);
criticNet = addLayers(criticNet, commonPath);
criticNet = connectLayers(criticNet, 'relu_s2', 'add/in1');
criticNet = connectLayers(criticNet, 'fc_a1', 'add/in2');

criticOpts = rlOptimizerOptions('LearnRate', 1e-3, 'GradientThreshold', 1);
critic = rlQValueFunction(criticNet, obsInfo, actInfo, ...
    'ObservationInputNames', 'state', 'ActionInputNames', 'action');

% Actor Network
actorNet = [
    featureInputLayer(2, 'Normalization', 'none', 'Name', 'state')
    fullyConnectedLayer(128, 'Name', 'fc1')
    reluLayer('Name', 'relu1')
    fullyConnectedLayer(64, 'Name', 'fc2')
    reluLayer('Name', 'relu2')
    fullyConnectedLayer(32, 'Name', 'fc3')
    reluLayer('Name', 'relu3')
    fullyConnectedLayer(2, 'Name', 'fc_out')
    tanhLayer('Name', 'tanh')];

actorOpts = rlOptimizerOptions('LearnRate', 5e-4, 'GradientThreshold', 1);
actor = rlContinuousDeterministicActor(actorNet, obsInfo, actInfo);

% DDPG Agent
agentOpts = rlDDPGAgentOptions(...
    'SampleTime', 1, ...
    'DiscountFactor', 0.95, ...
    'MiniBatchSize', 64, ...
    'ExperienceBufferLength', 1e5, ...
    'TargetSmoothFactor', 1e-3, ...
    'SaveExperienceBufferWithAgent', false);

agentOpts.NoiseOptions.Variance = 0.4;
agentOpts.NoiseOptions.VarianceDecayRate = 1e-4;
agentOpts.NoiseOptions.VarianceMin = 0.05;
agentOpts.ActorOptimizerOptions = actorOpts;
agentOpts.CriticOptimizerOptions = criticOpts;

agent = rlDDPGAgent(actor, critic, agentOpts);

disp('******  DDPG Agent created  ******');

%% 4. TRAIN RL AGENT
disp('3. Training RL Agent...');

maxEpisodes = 100;
maxStepsPerEpisode = 15;
TargetPopulationSize = 20;

trainOpts = rlTrainingOptions(...
    'MaxEpisodes', maxEpisodes, ...
    'MaxStepsPerEpisode', maxStepsPerEpisode, ...
    'Verbose', false, ...
    'Plots', 'training-progress', ...
    'StopTrainingCriteria', 'EpisodeCount', ...
    'StopTrainingValue', maxEpisodes, ...
    'ScoreAveragingWindowLength', 10);

try
    trainingInfo = train(agent, env, trainOpts);
    disp('******  Training completed  ******');
catch ME
    warning('Training error: %s', ME.message);
end

%% 5. EXTRACT SAMPLES
disp('4. Extracting best samples...');

sampleHistory = env.SampleHistory;
costHistory = env.CostHistory;
MAX_PENALTY = env.MaxPenalty;

if isempty(sampleHistory)
    warning('No samples collected. Using random initialization.');
    InitialPopulation = LB + (UB - LB) .* rand(TargetPopulationSize, 2);
else
    validIdx = costHistory < MAX_PENALTY;
    validSamples = sampleHistory(validIdx, :);
    validCosts = costHistory(validIdx);
    numValid = sum(validIdx);
    
    disp(['   Found ', num2str(numValid), ' valid samples.']);
    
    if numValid < TargetPopulationSize
        warning('Padding with random samples.');
        randomSamples = LB + (UB - LB) .* rand(TargetPopulationSize - numValid, 2);
        InitialPopulation = [validSamples; randomSamples];
    else
        [~, sortIdx] = sort(validCosts, 'ascend');
        InitialPopulation = validSamples(sortIdx(1:TargetPopulationSize), :);
        
        disp('   Top 5 samples:');
        for i = 1:min(5, numValid)
            fprintf('     #%d: Q=%.2e, R=%.2e, Cost=%.4f\n', ...
                i, 10^validSamples(sortIdx(i),1), ...
                10^validSamples(sortIdx(i),2), validCosts(sortIdx(i)));
        end
    end
end

disp(['GA population: ', num2str(size(InitialPopulation, 1)), ' individuals.']);

%% 6. GENETIC ALGORITHM
disp('5. Running Genetic Algorithm...');

gaOpts = optimoptions('ga', ...
    'PopulationSize', TargetPopulationSize, ...
    'InitialPopulationMatrix', InitialPopulation, ...
    'MaxGenerations', 60, ...
    'FunctionTolerance', 1e-6, ...
    'MaxStallGenerations', 15, ...
    'Display', 'iter', ...
    'PlotFcn', @gaplotbestf, ...
    'UseParallel', false);

costFcn = @(x) evaluateLQR(x, A, Bc, E, F_MR_MAX, input_excite, ...
    N_STATES, N_CONTROLS, ModelName);

[BestPos_log, BestCost] = ga(costFcn, 2, [], [], [], [], LB, UB, [], gaOpts);

%% 7. RESULTS
disp(' ');
disp('========================================================');
disp('  OPTIMIZATION RESULTS');
disp('========================================================');

Q_param_RLGA = 10^BestPos_log(1);
R_param_RLGA = 10^BestPos_log(2);
Q_RLGA = Q_param_RLGA * eye(N_STATES);
R_RLGA = R_param_RLGA * eye(N_CONTROLS);
K_RLGA = lqr(A, Bc, Q_RLGA, R_RLGA);

fprintf('Optimal Q Parameter (log10): %.4f\n', BestPos_log(1));
fprintf('Optimal R Parameter (log10): %.4f\n', BestPos_log(2));
fprintf('Optimal Q_param: %.4e\n', Q_param_RLGA);
fprintf('Optimal R_param: %.4e\n', R_param_RLGA);
fprintf('Minimum Cost (Max Drift): %.6f m\n', BestCost);
disp('========================================================');

%% 8. FINAL SIMULATION
disp('7. Running final simulation...');

assignin('base', 'K_lqr_RLGA', K_RLGA);
assignin('base', 'Q_optimal', Q_RLGA);
assignin('base', 'R_optimal', R_RLGA);
assignin('base', 'F_MR_MAX', F_MR_MAX);
assignin('base', 'F_MR_MIN', F_MR_MIN);
assignin('base', 'input_excite', input_excite);
assignin('base', 'A', A);
assignin('base', 'Bc', Bc);
assignin('base', 'E', E);

if bdIsLoaded(ModelName) || ~isempty(which(ModelName))
    SimTime = input_excite.Time(end);
    try
        simOut = sim(ModelName, 'SimulationMode', 'normal', ...
            'StopTime', num2str(SimTime), 'SrcWorkspace', 'base');
        disp('Simulation completed.');
    catch ME
        warning('Simulation failed: %s', ME.message);
    end
else
    warning('Simulink model %s not found.', ModelName);
end

disp('Optimization complete!');

%%  HELPER FUNCTION: Create Environment Class File
function createEnvironmentClass()
    % Create the environment class file if it doesn't exist
    
    classContent = [...
'classdef LQROptimizationEnv < rl.env.MATLABEnvironment', newline, ...
'    properties', newline, ...
'        LB, UB, CurrentState, StepCount, MaxSteps', newline, ...
'        A, Bc, E, F_MR_MAX, input_excite, N_STATES, N_CONTROLS, ModelName', newline, ...
'        MaxPenalty = 1e3', newline, ...
'        StepScale = 0.3', newline, ...
'        SampleHistory = []', newline, ...
'        CostHistory = []', newline, ...
'    end', newline, newline, ...
'    methods', newline, ...
'        function this = LQROptimizationEnv(LB, UB, A, Bc, E, F_MR_MAX, input_excite, N_STATES, N_CONTROLS, ModelName)', newline, ...
'            ObservationInfo = rlNumericSpec([2 1], ''LowerLimit'', LB'', ''UpperLimit'', UB'');', newline, ...
'            ObservationInfo.Name = ''LQR Parameters'';', newline, ...
'            ActionInfo = rlNumericSpec([2 1], ''LowerLimit'', -1*ones(2,1), ''UpperLimit'', 1*ones(2,1));', newline, ...
'            ActionInfo.Name = ''Parameter Adjustment'';', newline, ...
'            this = this@rl.env.MATLABEnvironment(ObservationInfo, ActionInfo);', newline, ...
'            this.LB = LB; this.UB = UB; this.A = A; this.Bc = Bc; this.E = E;', newline, ...
'            this.F_MR_MAX = F_MR_MAX; this.input_excite = input_excite;', newline, ...
'            this.N_STATES = N_STATES; this.N_CONTROLS = N_CONTROLS;', newline, ...
'            this.ModelName = ModelName; this.MaxSteps = 15;', newline, ...
'        end', newline, newline, ...
'        function [observation, reward, isDone, loggedSignals] = step(this, action)', newline, ...
'            action = double(action(:));', newline, ...
'            nextState = this.CurrentState + this.StepScale * action;', newline, ...
'            nextState = max(this.LB'', min(this.UB'', nextState));', newline, ...
'            cost = this.evaluateLQR(nextState'');', newline, ...
'            this.SampleHistory = [this.SampleHistory; nextState''];', newline, ...
'            this.CostHistory = [this.CostHistory; cost];', newline, ...
'            if cost >= this.MaxPenalty', newline, ...
'                reward = -100;', newline, ...
'            else', newline, ...
'                reward = -cost * 10;', newline, ...
'            end', newline, ...
'            this.CurrentState = nextState;', newline, ...
'            this.StepCount = this.StepCount + 1;', newline, ...
'            isDone = (this.StepCount >= this.MaxSteps);', newline, ...
'            observation = nextState;', newline, ...
'            loggedSignals.Cost = cost; loggedSignals.State = nextState;', newline, ...
'        end', newline, newline, ...
'        function initialObservation = reset(this)', newline, ...
'            if rand() < 0.2', newline, ...
'                this.CurrentState = ((this.LB + this.UB) / 2)'';', newline, ...
'            else', newline, ...
'                this.CurrentState = (this.LB + (this.UB - this.LB) .* rand(1, 2))'';', newline, ...
'            end', newline, ...
'            this.StepCount = 0;', newline, ...
'            initialObservation = this.CurrentState;', newline, ...
'        end', newline, ...
'    end', newline, newline, ...
'    methods (Access = private)', newline, ...
'        function cost = evaluateLQR(this, X_log)', newline, ...
'            Q_param = 10^X_log(1); R_param = 10^X_log(2);', newline, ...
'            Q_matrix = Q_param * eye(this.N_STATES);', newline, ...
'            R_matrix = R_param * eye(this.N_CONTROLS);', newline, ...
'            try', newline, ...
'                K_gain = lqr(this.A, this.Bc, Q_matrix, R_matrix);', newline, ...
'            catch', newline, ...
'                cost = this.MaxPenalty; return;', newline, ...
'            end', newline, ...
'            F_MR_MIN = -this.F_MR_MAX;', newline, ...
'            assignin(''base'', ''K_lqr_RLGA'', K_gain);', newline, ...
'            assignin(''base'', ''F_MR_MAX'', this.F_MR_MAX);', newline, ...
'            assignin(''base'', ''F_MR_MIN'', F_MR_MIN);', newline, ...
'            assignin(''base'', ''input_excite'', this.input_excite);', newline, ...
'            assignin(''base'', ''A'', this.A);', newline, ...
'            assignin(''base'', ''Bc'', this.Bc);', newline, ...
'            assignin(''base'', ''E'', this.E);', newline, ...
'            try', newline, ...
'                simOut = sim(this.ModelName, ''SimulationMode'', ''normal'', ''SrcWorkspace'', ''base'', ''ReturnWorkspaceOutputs'', ''on'');', newline, ...
'            catch', newline, ...
'                cost = this.MaxPenalty; return;', newline, ...
'            end', newline, ...
'            try', newline, ...
'                X_controlled = simOut.get(''OutRLGA'');', newline, ...
'                if isa(X_controlled, ''timeseries'')', newline, ...
'                    Displacements = X_controlled.Data(:, 1:this.N_STATES/2);', newline, ...
'                else', newline, ...
'                    cost = this.MaxPenalty; return;', newline, ...
'                end', newline, ...
'                n_stories = size(Displacements, 2);', newline, ...
'                Drifts = zeros(size(Displacements));', newline, ...
'                Drifts(:, 1) = Displacements(:, 1);', newline, ...
'                for i = 2:n_stories', newline, ...
'                    Drifts(:, i) = Displacements(:, i) - Displacements(:, i-1);', newline, ...
'                end', newline, ...
'                cost = max(max(abs(Drifts)));', newline, ...
'                if isnan(cost) || isinf(cost) || cost < 0', newline, ...
'                    cost = this.MaxPenalty;', newline, ...
'                end', newline, ...
'            catch', newline, ...
'                cost = this.MaxPenalty;', newline, ...
'            end', newline, ...
'        end', newline, ...
'    end', newline, ...
'end'];
    
    % Write to file
    fid = fopen('LQROptimizationEnv.m', 'w');
    fprintf(fid, '%s', classContent);
    fclose(fid);
    
    disp('Environment class file created: LQROptimizationEnv.m');
end

%%  STANDALONE EVALUATION FUNCTION FOR GA
function cost = evaluateLQR(X_log, A, Bc, E, F_MR_MAX, input_excite, N_STATES, N_CONTROLS, ModelName)
    MAX_PENALTY = 1e3;
    Q_param = 10^X_log(1); R_param = 10^X_log(2);
    Q_matrix = Q_param * eye(N_STATES); R_matrix = R_param * eye(N_CONTROLS);
    try
        K_gain = lqr(A, Bc, Q_matrix, R_matrix);
    catch
        cost = MAX_PENALTY; return;
    end
    F_MR_MIN = -F_MR_MAX;
    assignin('base', 'K_lqr_RLGA', K_gain);
    assignin('base', 'F_MR_MAX', F_MR_MAX);
    assignin('base', 'F_MR_MIN', F_MR_MIN);
    assignin('base', 'input_excite', input_excite);
    assignin('base', 'A', A); assignin('base', 'Bc', Bc); assignin('base', 'E', E);
    try
        simOut = sim(ModelName, 'SimulationMode', 'normal', 'SrcWorkspace', 'base', ...
            'ReturnWorkspaceOutputs', 'on', 'CaptureErrors', 'on');
    catch
        cost = MAX_PENALTY; return;
    end
    try
        X_controlled = simOut.get('OutRLGA');
        if isa(X_controlled, 'timeseries')
            Displacements = X_controlled.Data(:, 1:N_STATES/2);
        else
            cost = MAX_PENALTY; return;
        end
        n_stories = size(Displacements, 2); Drifts = zeros(size(Displacements));
        Drifts(:, 1) = Displacements(:, 1);
        for i = 2:n_stories
            Drifts(:, i) = Displacements(:, i) - Displacements(:, i-1);
        end
        cost = max(max(abs(Drifts)));
        if isnan(cost) || isinf(cost) || cost < 0
            cost = MAX_PENALTY;
        end
    catch
        cost = MAX_PENALTY;
    end
end